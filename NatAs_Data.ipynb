{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.OutputArea.prototype._should_scroll = function(lines) {\n  return false;\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "  return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import scipy as sp\n",
    "from scipy import sparse, signal, stats, interpolate\n",
    "from astropy import convolution as conv\n",
    "import numpy as np\n",
    "from numpy.fft import fft, ifft\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors as clrs\n",
    "import pandas as pd \n",
    "import time\n",
    "import datetime\n",
    "import calendar\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "import networkx as nx\n",
    "\n",
    "# import multiprocessing as mp\n",
    "# from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, r'functions')  # add to pythonpath\n",
    "\n",
    "figDPI = 100\n",
    "\n",
    "# Color definitions\n",
    "ClrS = (0.74, 0.00, 0.00)\n",
    "ClrN = (0.20, 0.56, 1.00)\n",
    "\n",
    "Clr = [(0.00, 0.00, 0.00),\n",
    "      (0.31, 0.24, 0.00),\n",
    "      (0.43, 0.16, 0.49),\n",
    "      (0.32, 0.70, 0.30),\n",
    "      (0.45, 0.70, 0.90),\n",
    "      (1.00, 0.82, 0.67)]\n",
    "\n",
    "# Font Size\n",
    "\n",
    "font = {'family': 'sans-serif',\n",
    "        'weight': 'normal',\n",
    "        'size'   : 30}\n",
    "\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading observations and observers\n",
    "\n",
    "The files used were downloaded from [SILSO's Group Number page](http://www.sidc.be/silso/groupnumberv3).  The current version used is JV_V1-12, but turned into a .csv file.  The original file contains leap years that should not be there.  February 29th for those years were removed by hand (1700, 1800, 1900).\n",
    "\n",
    "## Reading observations\n",
    "\n",
    "We recast the number of groups as a float so that we can use NaNs for days with missing observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GN_Dat = pd.read_csv(\n",
    "    \"input_data/GNObservations_JV_V1.22.csv\",\n",
    "    quotechar='\"',\n",
    "    header=15,\n",
    "    encoding=\"cp1252\",\n",
    ")\n",
    "\n",
    "GN_Dat['GROUPS'] = GN_Dat['GROUPS'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading observers\n",
    "\n",
    "We create a variable to store each unique observer.  **NOTE THAT CURRENTLY EACH STATION HAS _ONLY_ ONE OBSERVER SO WE DO ALL OUR LOGICAL OPERATIONS USING THE 'STATION' FIELD**.\n",
    "\n",
    "We also remove the '0' station which indicates a day without observations.\n",
    "\n",
    "The **_print_** statement on this cell can be use for reference as it shows the station number associated with each observer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GN_Obs = pd.read_csv(\n",
    "    \"input_data/GNObservers_JV_V1.22.csv\", quotechar='\"', encoding=\"cp1252\"\n",
    ")\n",
    "# print(GN_Obs[['INITIAL','FINAL','STATION','OBSERVER','TOT.OBS']].to_string())\n",
    "\n",
    "UnObs = np.unique(GN_Dat.STATION)\n",
    "UnObs = UnObs[UnObs>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Sunspot Group Number Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "JV_Dat = pd.read_csv('input_data/Vaquero_Uncertainty.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "# Adding 0.5 years to Jose's data to match the other people's standards\n",
    "JV_Dat['Year'] = JV_Dat['Year']+0.5\n",
    "# Combining limits to create a single S\n",
    "JV_Dat['S'] = (JV_Dat['S2'] + JV_Dat['S1'])/2\n",
    "JV_Dat = JV_Dat.loc[:,['Year','G','S','S1','S2']]\n",
    "\n",
    "# Interpolating Nadya's data to half years\n",
    "# High\n",
    "NZH_Dat = pd.read_csv('input_data/Zolotova_high.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "NZH_Dat = NZH_Dat.sort_values('Date')\n",
    "\n",
    "NZ_int = interpolate.splrep(NZH_Dat['Date'].values, NZH_Dat['G'].values, s=1)\n",
    "yr = np.arange(np.floor(np.min(NZH_Dat['Date'])),np.ceil(np.max(NZH_Dat['Date'])))+0.5\n",
    "G = interpolate.splev(yr, NZ_int)\n",
    "# Create new Zolotova database\n",
    "NZH_Dat = pd.DataFrame(data={'Date': yr, 'G': G})\n",
    "\n",
    "# Low\n",
    "NZL_Dat = pd.read_csv('input_data/Zolotova_low.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "NZL_Dat = NZL_Dat.sort_values('Date')\n",
    "\n",
    "NZ_int = interpolate.splrep(NZL_Dat['Date'].values, NZL_Dat['G'].values, s=7)\n",
    "yr = np.arange(np.floor(np.min(NZL_Dat['Date'])),np.ceil(np.max(NZL_Dat['Date'])))+0.5\n",
    "G = interpolate.splev(yr, NZ_int)\n",
    "# Create new Zolotova database\n",
    "NZL_Dat = pd.DataFrame(data={'Date': yr, 'G': G})\n",
    "\n",
    "# Mixed\n",
    "yr = np.intersect1d(NZH_Dat['Date'],NZL_Dat['Date'])\n",
    "G = (NZH_Dat.loc[np.logical_and(NZH_Dat['Date']>=np.min(yr),NZH_Dat['Date']<=np.max(yr)),'G'] +\n",
    "     NZL_Dat.loc[np.logical_and(NZL_Dat['Date']>=np.min(yr),NZL_Dat['Date']<=np.max(yr)),'G'] )/2\n",
    "\n",
    "S = (NZH_Dat.loc[np.logical_and(NZH_Dat['Date']>=np.min(yr),NZH_Dat['Date']<=np.max(yr)),'G'] -\n",
    "     NZL_Dat.loc[np.logical_and(NZL_Dat['Date']>=np.min(yr),NZL_Dat['Date']<=np.max(yr)),'G'] )/2\n",
    "\n",
    "MS = np.mean(np.abs((NZH_Dat.loc[np.logical_and(NZH_Dat['Date']>=np.min(yr),NZH_Dat['Date']<=1670),'G'] -\n",
    "     NZL_Dat.loc[np.logical_and(NZL_Dat['Date']>=np.min(yr),NZL_Dat['Date']<=1670),'G'] )/2))\n",
    "\n",
    "S[S<MS] = MS\n",
    "\n",
    "NZM_Dat = pd.DataFrame(data={'Date': yr, 'G': G, 'S':G*0+S})\n",
    "\n",
    "\n",
    "\n",
    "SS_Dat = pd.read_csv('input_data/GNbb2_y.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "# Removing zero uncertainties\n",
    "SS_Dat.loc[SS_Dat['S']==0,'S'] = np.min(SS_Dat.loc[SS_Dat['S']!=0,'S'])\n",
    "\n",
    "U_Dat = pd.read_csv('input_data/GNiu_y2.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "U_Dat = U_Dat.loc[:,['Year','G','S']]\n",
    "# Removing zero uncertainties\n",
    "U_Dat.loc[U_Dat['S']==0,'S'] = np.min(U_Dat.loc[U_Dat['S']!=0,'S'])\n",
    "\n",
    "\n",
    "\n",
    "HS_Dat = pd.read_csv('input_data/GNhs_y.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "# Changing the uncertainty of HS for the one in SS\n",
    "HS_Dat['S'] = SS_Dat.loc[0:HS_Dat.shape[0],'S']\n",
    "\n",
    "LO_Dat = pd.read_csv('input_data/Lockwood_etal_2014.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "# Changing the uncertainty of HS for the one in SS\n",
    "LO_Dat['S'] = SS_Dat.loc[0:LO_Dat.shape[0],'S']\n",
    "# Re-scaling to group number\n",
    "LO_Dat['G'] = LO_Dat['G']/12.08\n",
    "\n",
    "\n",
    "WSN_Dat = pd.read_csv('input_data/SN_y_tot_V2.0.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "WSN_Dat = WSN_Dat.loc[:,['Year','G','S']]\n",
    "# Removing zero uncertainties\n",
    "WSN_Dat.loc[WSN_Dat['S']<=0,'S'] = np.min(WSN_Dat.loc[WSN_Dat['S']!=0,'S'])\n",
    "# Re-scaling to group number\n",
    "WSN_Dat['G'] = WSN_Dat['G']/20\n",
    "WSN_Dat['S'] = WSN_Dat['S']/20\n",
    "\n",
    "Ch_Dat = pd.read_csv('input_data/Chatzistergos.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "Ch_Dat['S'] = (Ch_Dat['S2'] + Ch_Dat['S1'])/2\n",
    "\n",
    "IM_Dat = pd.read_csv('input_data/Ivanov_Miletsky.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "IM_Dat['Year'] = IM_Dat['Year']+0.5\n",
    "\n",
    "\n",
    "CL_Dat = pd.read_csv('input_data/Cliver_ling_2016.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "CL_Dat['Year'] = CL_Dat['Year']+0.5\n",
    "CL_Dat['G'] = CL_Dat['RGC']/12.08\n",
    "\n",
    "# Changing the uncertainty of CL for the one in SS\n",
    "CL_Dat['S'] = SS_Dat.loc[231:370,'S'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Butterfly Diagram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BflyMM = pd.read_csv('input_data/Sunspot_Latitudes_MM.csv', quotechar = '\"', encoding = 'cp1252',header=14)\n",
    "BflyMod = pd.read_csv('input_data/composite_bfly_all.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "BflyMod['ORDINAL'] = BflyMod.apply(lambda x: datetime.date(int(x['YEAR']),int(x['MONTH']),int(x['DAY'])).toordinal(),axis=1)\n",
    "\n",
    "BflyMM1 = pd.read_csv('input_data/Bfly_Gissendi_Malapert_Macgraf.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "\n",
    "BflyHrt = pd.read_csv('input_data/Bfly_harriot.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "BflyHrt['ORDINAL'] = BflyHrt.apply(lambda x: datetime.date(int(x['Year']),int(x['Month']),int(x['Day'])).toordinal(),axis=1)\n",
    "BflyHrt['FRACYEAR'] = BflyHrt.apply(lambda x: x['Year'].astype(int)\n",
    "                                            + (  datetime.date(x['Year'].astype(int),x['Month'].astype(int),x['Day'].astype(int)).toordinal()\n",
    "                                               - datetime.date(x['Year'].astype(int),1,1).toordinal() )\n",
    "                                            / (  datetime.date(x['Year'].astype(int)+1,1,1).toordinal()\n",
    "                                               - datetime.date(x['Year'].astype(int),1,1).toordinal() )\n",
    "                                  ,axis=1)\n",
    "\n",
    "\n",
    "# BflyGal = pd.read_csv('input_data/Bfly_Galileo.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "BflyGal = pd.read_csv('input_data/Bfly_Galileo_Zolotova.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "\n",
    "BflyGal['ORDINAL'] = BflyGal.apply(lambda x: datetime.date(int(x['Year']),int(x['Month']),int(x['Day'])).toordinal(),axis=1)\n",
    "BflyGal['FRACYEAR'] = BflyGal.apply(lambda x: x['Year'].astype(int)\n",
    "                                            + (  datetime.date(x['Year'].astype(int),x['Month'].astype(int),x['Day'].astype(int)).toordinal()\n",
    "                                               - datetime.date(x['Year'].astype(int),1,1).toordinal() )\n",
    "                                            / (  datetime.date(x['Year'].astype(int)+1,1,1).toordinal()\n",
    "                                               - datetime.date(x['Year'].astype(int),1,1).toordinal() )\n",
    "                                  ,axis=1)\n",
    "\n",
    "BflyHvl = pd.read_csv('input_data/Bfly_hevelius_JV.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "\n",
    "BflySchn = pd.read_csv('input_data/Bfly_scheiner.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "BflySchn['ORDINAL'] = BflySchn.apply(lambda x: datetime.date(int(x['Year']),int(x['Month']),int(x['Day'])).toordinal(),axis=1)\n",
    "BflySchn['FRACYEAR'] = BflySchn.apply(lambda x: x['Year'].astype(int)\n",
    "                                            + (  datetime.date(x['Year'].astype(int),x['Month'].astype(int),x['Day'].astype(int)).toordinal()\n",
    "                                               - datetime.date(x['Year'].astype(int),1,1).toordinal() )\n",
    "                                            / (  datetime.date(x['Year'].astype(int)+1,1,1).toordinal()\n",
    "                                               - datetime.date(x['Year'].astype(int),1,1).toordinal() )\n",
    "                                  ,axis=1)\n",
    "\n",
    "\n",
    "BflyStdc = pd.read_csv('input_data/Bfly_staudacher.csv', quotechar = '\"', encoding = 'cp1252')\n",
    "BflyStdc['ORDINAL'] = BflyStdc.apply(lambda x: datetime.date(int(x['Year']),int(x['Month']),int(x['Day'])).toordinal(),axis=1)\n",
    "BflyStdc['FRACYEAR'] = BflyStdc.apply(lambda x: x['Year'].astype(int)\n",
    "                                            + (  datetime.date(x['Year'].astype(int),x['Month'].astype(int),x['Day'].astype(int)).toordinal()\n",
    "                                               - datetime.date(x['Year'].astype(int),1,1).toordinal() )\n",
    "                                            / (  datetime.date(x['Year'].astype(int)+1,1,1).toordinal()\n",
    "                                               - datetime.date(x['Year'].astype(int),1,1).toordinal() )\n",
    "                                  ,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-allocating Variables\n",
    "\n",
    "We create a variable to store each unique observer.  **NOTE THAT CURRENTLY EACH STATION HAS _ONLY_ ONE OBSERVER SO WE DO ALL OUR LOGICAL OPERATIONS USING THE 'STATION' FIELD**.\n",
    "\n",
    "We also remove the '0' station which indicates a day without observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(715,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UnObs = np.unique(GN_Dat.STATION)\n",
    "UnObs = UnObs[UnObs>0]\n",
    "UnObs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day adjacency tolerance\n",
    "\n",
    "Since the amount of groups visible in the Sun don't change radically from day to day, we specify a window of tolerance in which two observations are considered to be _\"simultaneous\"_.  First priority is given to observations taken on the same calendar date, then to observations within one day are considered, then within two days, and so on until reaching the size of the tolerance window.  The process is ran simultaneously for prior and following days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DayTol = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum days of overlap\n",
    "\n",
    "In order to establish a link between two observers in the network, we require to have minimum amount of days of overlap (considering as overlapping days days within DayTol of each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinOvr = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the adjacency matrix\n",
    "\n",
    "Now we go through all observers **_(stations)_** executing the following algorithm:\n",
    "\n",
    "1. Pick an observer _(station)_.\n",
    "2. Find all observers _(stations)_ that have overlapping observations within DayTol.\n",
    "3. Go through all overlaping observers _(stations)_ and define the adjacency matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Minimum number of different group numbers to consider an observer valid\n",
    "minObVal = 4\n",
    "\n",
    "#Number of days of overlap\n",
    "LnkDays = sp.sparse.lil_matrix((UnObs.shape[0], UnObs.shape[0]))\n",
    "\n",
    "pos = {} #Dictionary with positions for plotting the network\n",
    "for iObs1 in range(UnObs.shape[0]):\n",
    "\n",
    "    #---------------------\n",
    "    # 1. Pick an observer    \n",
    "    #---------------------\n",
    "    \n",
    "    Obs1 = GN_Dat[GN_Dat['STATION'] == UnObs[iObs1]]\n",
    "    \n",
    "    #Update position dictionary of for plotting\n",
    "    pos.update({iObs1: [ np.mean(Obs1.FRACYEAR), np.random.random_sample()]})\n",
    "    \n",
    "    # Create an array of valid date of observations including +/- DayTol\n",
    "    DatObs1 = Obs1.ORDINAL\n",
    "    TmpList = DatObs1\n",
    "    for iDat in range(DayTol):\n",
    "        TmpList = pd.concat([TmpList,DatObs1 + iDat + 1])\n",
    "        TmpList = pd.concat([TmpList,DatObs1 - iDat - 1])    \n",
    "    DatObs1 = np.unique(TmpList)\n",
    "\n",
    "    \n",
    "    #---------------------\n",
    "    # 2. Find all observers that have overlapping observations within DayTol\n",
    "    #---------------------\n",
    "    \n",
    "    ObsOvrlp = np.in1d(GN_Dat.ORDINAL, DatObs1)\n",
    "    ObsOvrlp = np.unique(GN_Dat.STATION[ObsOvrlp])\n",
    "    ObsOvrlp = ObsOvrlp[np.logical_and(ObsOvrlp != 0, ObsOvrlp != UnObs[iObs1])]\n",
    "    \n",
    "    # Void observer if it doesn't have enough values\n",
    "    if np.unique(Obs1['GROUPS']).shape[0] < minObVal:\n",
    "        ObsOvrlp = np.array([]) \n",
    "    \n",
    "    #---------------------\n",
    "    # 3. Go through all overlaping observers and define the adjacency matrices.\n",
    "    #---------------------\n",
    "    \n",
    "    for iObsOvrlp in range(ObsOvrlp.shape[0]):\n",
    "        \n",
    "        #Determine the overlapping observer\n",
    "        iObs2 = (UnObs==ObsOvrlp[iObsOvrlp]).nonzero()\n",
    "        Obs2 = GN_Dat[GN_Dat.STATION == ObsOvrlp[iObsOvrlp]]\n",
    "        \n",
    "        # Remove zeros to include only matches that could potentially signify a useful connection\n",
    "        #Obs2 = Obs2[Obs2['GROUPS'] > 0]\n",
    "        \n",
    "        DatObs2 = Obs2.ORDINAL\n",
    "        \n",
    "        \n",
    "        # Void observer if it doesn't have enough values\n",
    "        if np.unique(Obs2['GROUPS']).shape[0] < minObVal:\n",
    "            DatObs2 = np.array([])        \n",
    "        \n",
    "        #Find the dates in common (including days within DayTol)\n",
    "        DatIntrsct = np.intersect1d(DatObs1,DatObs2)\n",
    "        \n",
    "        #Fill the days of overlap adjacency matrix\n",
    "        if DatIntrsct.shape[0] >= MinOvr:\n",
    "            LnkDays[iObs1,iObs2] = DatIntrsct.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blending Group Number Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Time Grid\n",
    "Y1 = 1600  # First year of the window of interest\n",
    "Y2 = 2020  # Second year of the window of interest\n",
    "dYr = 0.25 # Coarness of the year grid\n",
    "YrBl = np.arange(Y1+0.5-dYr/2,Y2+dYr,dYr)\n",
    "\n",
    "# Defining group Grid\n",
    "dG = 0.25  # Coarness of the Group grid\n",
    "Gmax = 17 # Maximum group number to consider\n",
    "GrpsBl = np.arange(0,Gmax+dG,dG)\n",
    "\n",
    "# Creating Blending Matrix\n",
    "GrpsBM = np.zeros((YrBl.shape[0],GrpsBl.shape[0]))\n",
    "\n",
    "# Interpolating to grid\n",
    "\n",
    "## Svalgaard and Schatten\n",
    "# Create array with missing dates\n",
    "tmpyr = YrBl.copy()+dYr/2\n",
    "tmpyr = tmpyr[np.logical_and(tmpyr>np.min(SS_Dat['Year']),tmpyr<np.max(SS_Dat['Year']))]\n",
    "MisYr = np.setdiff1d(tmpyr,SS_Dat['Year'])\n",
    "\n",
    "# Create temporary dataframe to interpolate and append\n",
    "pdAppnd = np.zeros((MisYr.shape[0], SS_Dat.shape[1]))\n",
    "pdAppnd.fill(np.nan)\n",
    "pdAppnd[:,0] = MisYr\n",
    "pdAppnd = pd.DataFrame(pdAppnd, columns=['Year','G','S'])\n",
    "\n",
    "# Interpolate missing values\n",
    "tmp_int = interpolate.splrep(SS_Dat['Year'].values, SS_Dat['G'].values, s=1)\n",
    "pdAppnd['G'] = interpolate.splev(pdAppnd['Year'], tmp_int)\n",
    "tmp_int = interpolate.splrep(SS_Dat['Year'].values, SS_Dat['S'].values, s=1)\n",
    "pdAppnd['S'] = interpolate.splev(pdAppnd['Year'], tmp_int)\n",
    "\n",
    "# Append\n",
    "# SS_DatInt = SS_Dat.concat(pdAppnd)\n",
    "SS_DatInt = pd.concat([SS_Dat, pdAppnd])\n",
    "SS_DatInt = SS_DatInt.sort_values('Year').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Usoskin etal\n",
    "# Create array with missing dates\n",
    "tmpyr = YrBl.copy()+dYr/2\n",
    "tmpyr = tmpyr[np.logical_and(tmpyr>np.min(U_Dat['Year']),tmpyr<np.max(U_Dat['Year']))]\n",
    "MisYr = np.setdiff1d(tmpyr,U_Dat['Year'])\n",
    "\n",
    "# Create temporary dataframe to interpolate and append\n",
    "pdAppnd = np.zeros((MisYr.shape[0], U_Dat.shape[1]))\n",
    "pdAppnd.fill(np.nan)\n",
    "pdAppnd[:,0] = MisYr\n",
    "# pdAppnd = pd.DataFrame(pdAppnd, columns=['Year','G','S1','S2','S'])\n",
    "pdAppnd = pd.DataFrame(pdAppnd, columns=['Year','G','S'])\n",
    "\n",
    "# Interpolate missing values\n",
    "tmp_int = interpolate.splrep(U_Dat.loc[np.isfinite(U_Dat['G']),'Year'].values, U_Dat.loc[np.isfinite(U_Dat['G']),'G'].values, s=2)\n",
    "pdAppnd['G'] = interpolate.splev(pdAppnd['Year'], tmp_int)\n",
    "tmp_int = interpolate.splrep(U_Dat.loc[np.isfinite(U_Dat['G']),'Year'].values, U_Dat.loc[np.isfinite(U_Dat['G']),'S'].values, s=2)\n",
    "pdAppnd['S'] = interpolate.splev(pdAppnd['Year'], tmp_int)\n",
    "\n",
    "# Append\n",
    "# U_DatInt = U_Dat.concat(pdAppnd)\n",
    "U_DatInt = pd.concat([U_Dat, pdAppnd])\n",
    "U_DatInt = U_DatInt.sort_values('Year').reset_index(drop=True)\n",
    "\n",
    "# Remove NaN intervals\n",
    "for i in range(0, U_Dat.shape[0]):\n",
    "    if np.isnan(U_Dat['G'].values[i]):\n",
    "        U_DatInt.loc[np.logical_and(U_DatInt['Year']>U_Dat['Year'].values[i]-1, \n",
    "                      JU_DatInt['Year']<U_Dat['Year'].values[i]+1), ['G','S']] = np.nan\n",
    "\n",
    "\n",
    "## Vaquero etal\n",
    "# Create array with missing dates\n",
    "tmpyr = YrBl.copy()+dYr/2\n",
    "tmpyr = tmpyr[np.logical_and(tmpyr>np.min(JV_Dat['Year']),tmpyr<np.max(JV_Dat['Year']))]\n",
    "MisYr = np.setdiff1d(tmpyr,JV_Dat['Year'])\n",
    "\n",
    "# Create temporary dataframe to interpolate and append\n",
    "pdAppnd = np.zeros((MisYr.shape[0], JV_Dat.shape[1]))\n",
    "pdAppnd.fill(np.nan)\n",
    "pdAppnd[:,0] = MisYr\n",
    "pdAppnd = pd.DataFrame(pdAppnd, columns=['Year','G','S1','S2','S'])\n",
    "\n",
    "# Interpolate missing values\n",
    "tmp_int = interpolate.splrep(JV_Dat.loc[np.isfinite(JV_Dat['G']),'Year'].values, JV_Dat.loc[np.isfinite(JV_Dat['G']),'G'].values, s=1)\n",
    "pdAppnd['G'] = interpolate.splev(pdAppnd['Year'], tmp_int)\n",
    "tmp_int = interpolate.splrep(JV_Dat.loc[np.isfinite(JV_Dat['G']),'Year'].values, JV_Dat.loc[np.isfinite(JV_Dat['G']),'S'].values, s=1)\n",
    "pdAppnd['S'] = interpolate.splev(pdAppnd['Year'], tmp_int)\n",
    "\n",
    "# Append\n",
    "# JV_DatInt = JV_Dat.concat(pdAppnd)\n",
    "JV_DatInt = pd.concat([JV_Dat, pdAppnd])\n",
    "JV_DatInt = JV_DatInt.sort_values('Year').reset_index(drop=True)\n",
    "\n",
    "# Remove NaN intervals\n",
    "for i in range(0, JV_Dat.shape[0]):\n",
    "    if np.isnan(JV_Dat['G'].values[i]):\n",
    "        JV_DatInt.loc[np.logical_and(JV_DatInt['Year']>JV_Dat['Year'].values[i]-1, \n",
    "                      JV_DatInt['Year']<JV_Dat['Year'].values[i]+1), ['G','S']] = np.nan\n",
    "\n",
    "\n",
    "## Chatzistergos etal\n",
    "# Create array with missing dates\n",
    "tmpyr = YrBl.copy()+dYr/2\n",
    "tmpyr = tmpyr[np.logical_and(tmpyr>np.min(Ch_Dat['Year']),tmpyr<np.max(Ch_Dat['Year']))]\n",
    "MisYr = np.setdiff1d(tmpyr,Ch_Dat['Year'])\n",
    "\n",
    "# Create temporary dataframe to interpolate and append\n",
    "pdAppnd = np.zeros((MisYr.shape[0], Ch_Dat.shape[1]))\n",
    "pdAppnd.fill(np.nan)\n",
    "pdAppnd[:,0] = MisYr\n",
    "pdAppnd = pd.DataFrame(pdAppnd, columns=['Year','G','S1','S2','S'])\n",
    "\n",
    "# Interpolate missing values\n",
    "tmp_int = interpolate.splrep(Ch_Dat.loc[np.isfinite(Ch_Dat['G']),'Year'].values, Ch_Dat.loc[np.isfinite(Ch_Dat['G']),'G'].values, s=1)\n",
    "pdAppnd['G'] = interpolate.splev(pdAppnd['Year'], tmp_int)\n",
    "tmp_int = interpolate.splrep(Ch_Dat.loc[np.isfinite(Ch_Dat['G']),'Year'].values, Ch_Dat.loc[np.isfinite(Ch_Dat['G']),'S'].values, s=1)\n",
    "pdAppnd['S'] = interpolate.splev(pdAppnd['Year'], tmp_int)\n",
    "\n",
    "# Append\n",
    "# Ch_DatInt = Ch_Dat.concat(pdAppnd)\n",
    "Ch_DatInt = pd.concat([Ch_Dat, pdAppnd])\n",
    "Ch_DatInt = Ch_DatInt.sort_values('Year').reset_index(drop=True)\n",
    "\n",
    "# Remove NaN intervals\n",
    "for i in range(0, Ch_Dat.shape[0]):\n",
    "    if np.isnan(Ch_Dat['G'].values[i]):\n",
    "        Ch_DatInt.loc[np.logical_and(Ch_DatInt['Year']>Ch_Dat['Year'].values[i]-1, \n",
    "                      Ch_DatInt['Year']<Ch_Dat['Year'].values[i]+1), ['G','S']] = np.nan\n",
    "\n",
    "\n",
    "## Ivanov Miletsky\n",
    "# Create array with missing dates\n",
    "tmpyr = YrBl.copy()+dYr/2\n",
    "tmpyr = tmpyr[np.logical_and(tmpyr>np.min(IM_Dat['Year']),tmpyr<np.max(IM_Dat['Year']))]\n",
    "MisYr = np.setdiff1d(tmpyr,IM_Dat['Year'])\n",
    "\n",
    "# Create temporary dataframe to interpolate and append\n",
    "pdAppnd = np.zeros((MisYr.shape[0], IM_Dat.shape[1]))\n",
    "pdAppnd.fill(np.nan)\n",
    "pdAppnd[:,0] = MisYr\n",
    "pdAppnd = pd.DataFrame(pdAppnd, columns=['Year','G','S'])\n",
    "\n",
    "# Interpolate missing values\n",
    "tmp_int = interpolate.splrep(IM_Dat.loc[np.isfinite(IM_Dat['G']),'Year'].values, IM_Dat.loc[np.isfinite(IM_Dat['G']),'G'].values, s=1)\n",
    "pdAppnd['G'] = interpolate.splev(pdAppnd['Year'], tmp_int)\n",
    "tmp_int = interpolate.splrep(IM_Dat.loc[np.isfinite(IM_Dat['G']),'Year'].values, IM_Dat.loc[np.isfinite(IM_Dat['G']),'S'].values, s=1)\n",
    "pdAppnd['S'] = interpolate.splev(pdAppnd['Year'], tmp_int)\n",
    "\n",
    "# Append\n",
    "# IM_DatInt = IM_Dat.concat(pdAppnd)\n",
    "IM_DatInt = pd.concat([IM_Dat, pdAppnd])\n",
    "IM_DatInt = IM_DatInt.sort_values('Year').reset_index(drop=True)\n",
    "\n",
    "# Remove NaN intervals\n",
    "for i in range(0, IM_Dat.shape[0]):\n",
    "    if np.isnan(IM_Dat['G'].values[i]):\n",
    "        IM_DatInt.loc[np.logical_and(IM_DatInt['Year']>IM_Dat['Year'].values[i]-1, \n",
    "                      IM_DatInt['Year']<IM_Dat['Year'].values[i]+1), ['G','S']] = np.nan\n",
    "\n",
    "# Filling the Blending Matrix\n",
    "for i in range(0,YrBl.shape[0]):\n",
    "    # Create temporary storage variable for blending\n",
    "    tmpBl = GrpsBl*0\n",
    "    \n",
    "    # Add Svalgaard and Schatten\n",
    "    if np.sum(np.logical_and(SS_DatInt['Year']>YrBl[i],SS_DatInt['Year']<YrBl[i]+1)) > 0:\n",
    "        # Find groups and error\n",
    "        tmpDat = SS_DatInt[np.logical_and(SS_DatInt['Year']>YrBl[i],SS_DatInt['Year']<YrBl[i]+1)]\n",
    "        # Add gaussian to temporary blending variable\n",
    "        Gauss =  sp.stats.norm(loc = tmpDat['G'].values[0], scale = tmpDat['S'].values[0])\n",
    "        tmpBl = tmpBl + Gauss.cdf(GrpsBl+1) - Gauss.cdf(GrpsBl)\n",
    "        \n",
    "    # Add Usoskin et al.\n",
    "    if np.sum(np.logical_and(U_DatInt['Year']>YrBl[i],U_DatInt['Year']<YrBl[i]+1)) > 0:\n",
    "        # Find groups and error\n",
    "        tmpDat = U_DatInt[np.logical_and(U_DatInt['Year']>YrBl[i],U_DatInt['Year']<YrBl[i]+1)]\n",
    "        # Add gaussian to temporary blending variable\n",
    "        Gauss =  sp.stats.norm(loc = tmpDat['G'].values[0], scale = tmpDat['S'].values[0])\n",
    "        tmpBl = tmpBl + Gauss.cdf(GrpsBl+1) - Gauss.cdf(GrpsBl)\n",
    "        \n",
    "        \n",
    "    # Add Vaquero et al.\n",
    "    if np.sum(np.logical_and(JV_DatInt['Year']>YrBl[i],JV_DatInt['Year']<YrBl[i]+1)) > 0:\n",
    "        # Find groups and error\n",
    "        tmpDat = JV_DatInt[np.logical_and(JV_DatInt['Year']>YrBl[i],JV_DatInt['Year']<YrBl[i]+1)]\n",
    "        \n",
    "        if np.isfinite(tmpDat['G'].values[0]):\n",
    "            # Add gaussian to temporary blending variable\n",
    "            Gauss =  sp.stats.norm(loc = tmpDat['G'].values[0], scale = tmpDat['S'].values[0])\n",
    "            tmpBl = tmpBl + Gauss.cdf(GrpsBl+1) - Gauss.cdf(GrpsBl)\n",
    "            \n",
    "            \n",
    "    # Add Chatzistergos et al.\n",
    "    if np.sum(np.logical_and(Ch_DatInt['Year']>YrBl[i],Ch_DatInt['Year']<YrBl[i]+1)) > 0:\n",
    "        # Find groups and error\n",
    "        tmpDat = Ch_DatInt[np.logical_and(Ch_DatInt['Year']>YrBl[i],Ch_DatInt['Year']<YrBl[i]+1)]\n",
    "        \n",
    "        if np.isfinite(tmpDat['G'].values[0]):\n",
    "            # Add gaussian to temporary blending variable\n",
    "            Gauss =  sp.stats.norm(loc = tmpDat['G'].values[0], scale = tmpDat['S'].values[0])\n",
    "            tmpBl = tmpBl + Gauss.cdf(GrpsBl+1) - Gauss.cdf(GrpsBl)            \n",
    "\n",
    "    # Add Ivanov & Miletsky.\n",
    "    if np.sum(np.logical_and(IM_DatInt['Year']>YrBl[i],IM_DatInt['Year']<YrBl[i]+1)) > 0:\n",
    "        # Find groups and error\n",
    "        tmpDat = IM_DatInt[np.logical_and(IM_DatInt['Year']>YrBl[i],IM_DatInt['Year']<YrBl[i]+1)]\n",
    "        \n",
    "        if np.isfinite(tmpDat['G'].values[0]):\n",
    "            # Add gaussian to temporary blending variable\n",
    "            Gauss =  sp.stats.norm(loc = tmpDat['G'].values[0], scale = tmpDat['S'].values[0])\n",
    "            tmpBl = tmpBl + Gauss.cdf(GrpsBl+1) - Gauss.cdf(GrpsBl)            \n",
    "        \n",
    "    GrpsBM[i,:] = tmpBl\n",
    "\n",
    "# Cleaning matrix of small values for plotting\n",
    "GrpsBM[GrpsBM==0] = np.nan    \n",
    "tmp = np.squeeze(GrpsBM.reshape((1,-1)))\n",
    "GrpsBM[GrpsBM<np.percentile(tmp[np.isfinite(tmp)],70)] = np.nan\n",
    "\n",
    "# print(Ch_Dat, Ch_DatInt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and plotting graph network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'networkx' has no attribute 'from_scipy_sparse_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Creating Graph Network\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ObsNtwrk \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_scipy_sparse_matrix\u001b[49m(LnkDays,create_using\u001b[38;5;241m=\u001b[39mnx\u001b[38;5;241m.\u001b[39mGraph())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Clusters existing in the network\u001b[39;00m\n\u001b[1;32m      5\u001b[0m Clst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(nx\u001b[38;5;241m.\u001b[39mconnected_components(ObsNtwrk), key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'networkx' has no attribute 'from_scipy_sparse_matrix'"
     ]
    }
   ],
   "source": [
    "#Creating Graph Network\n",
    "ObsNtwrk = nx.from_scipy_sparse_matrix(LnkDays,create_using=nx.Graph())\n",
    "\n",
    "#Clusters existing in the network\n",
    "Clst = sorted(nx.connected_components(ObsNtwrk), key = len, reverse=True)\n",
    "\n",
    "#Plotting Graph Network\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "# Size definitions\n",
    "dpi = 300\n",
    "pxx = 8600   # Horizontal size of each panel\n",
    "pxy = 1700    # Vertical size of each panel\n",
    "\n",
    "nph = 1      # Number of horizontal panels\n",
    "npv = 4      # Number of vertical panels\n",
    "\n",
    "# Padding\n",
    "padv  = 200 #Vertical padding in pixels\n",
    "padv2 = 0  #Vertical padding in pixels between panels\n",
    "padh  = 300 #Horizontal padding in pixels at the edge of the figure\n",
    "padh2 = 150 #Horizontal padding in pixels between panels\n",
    "\n",
    "# Figure sizes in pixels\n",
    "fszv = (npv*pxy + 2*padv + (npv-1)*padv2 )      #Vertical size of figure in pixels\n",
    "fszh = (nph*pxx + 2*padh + (nph-1)*padh2 )      #Horizontal size of figure in pixels\n",
    "\n",
    "# Conversion to relative unites\n",
    "ppxx   = pxx/fszh\n",
    "ppxy   = pxy/fszv\n",
    "ppadv  = padv/fszv     #Vertical padding in relative units\n",
    "ppadv2 = padv2/fszv    #Vertical padding in relative units\n",
    "ppadh  = padh/fszh     #Horizontal padding the edge of the figure in relative units\n",
    "ppadh2 = padh2/fszh    #Horizontal padding between panels in relative units\n",
    "\n",
    "\n",
    "## Start Figure\n",
    "Y1 = 1600\n",
    "Y2 = 2020\n",
    "\n",
    "fig = plt.figure(figsize=(fszh/dpi,fszv/dpi))\n",
    "\n",
    "ax1 = fig.add_axes([1.6*ppadh, ppadv+3*ppxy, ppxx, ppxy])\n",
    "\n",
    "#Colors\n",
    "cmap = plt.cm.get_cmap('Set1')\n",
    "\n",
    "#LineWidth\n",
    "LW = 5\n",
    "\n",
    "ax1.plot(HS_Dat['Year'],HS_Dat['G'], zorder=1, color=cmap(7), linewidth=LW)\n",
    "ax1.plot(LO_Dat['Year'],LO_Dat['G'], zorder=1, color='c',linestyle='-', linewidth=LW)\n",
    "ax1.plot(CL_Dat['Year'],CL_Dat['G'], zorder=1, color='m',linestyle='-', linewidth=LW)\n",
    "ax1.plot(NZM_Dat['Date'],NZM_Dat['G'], zorder=1, color=cmap(4),linestyle='-.', linewidth=LW)\n",
    "ax1.plot(SS_Dat['Year'],SS_Dat['G'], zorder=1, color=cmap(1),linestyle=':',linewidth=LW)\n",
    "ax1.plot(U_Dat['Year'],U_Dat['G'], zorder=1, color=cmap(6),linestyle='--', linewidth=LW)\n",
    "ax1.plot(IM_Dat['Year'],IM_Dat['G'], zorder=1, color=cmap(3),linestyle='--', linewidth=LW)\n",
    "ax1.plot(JV_Dat['Year'],JV_Dat['G'], zorder=1, color=cmap(0), linewidth=LW)\n",
    "ax1.plot(Ch_Dat['Year'],Ch_Dat['G'], zorder=1, color=cmap(2), linewidth=LW,linestyle='--')\n",
    "ax1.plot(WSN_Dat['Year'],WSN_Dat['G'], zorder=1, color='k',linestyle=':', linewidth=LW)\n",
    "\n",
    "leg = ax1.legend(('Hoyt & Schatten (1998)', 'Lockwood et al. (2014)', 'Cliver & Ling (2016)', 'Zolotova & Ponyavin (2015)','Svalgaard & Schatten (2016)', 'Willamo et al. (2017)'\n",
    "            , 'Ivanov & Miletsky (2017)','Vaquero et al. (2015)', 'Chatzistergos et al. (2017)','Wolf Sunspot Number/20\\nClette & Lefèvre (2016)'), \n",
    "           ncol = 4, loc = 2, fontsize=28, frameon=False, columnspacing=1)\n",
    "\n",
    "for i in range(0,4):\n",
    "    t = leg.get_texts()[i]\n",
    "    t.set_fontproperties(t.get_fontproperties())\n",
    "    t.set_color('r')\n",
    "    \n",
    "t = leg.get_texts()[9]\n",
    "t.set_fontproperties(t.get_fontproperties())\n",
    "t.set_color('r')    \n",
    "\n",
    "alphUn = 0.4\n",
    "\n",
    "ax1.fill_between(U_Dat['Year'],U_Dat['G']-U_Dat['S'], U_Dat['G']+U_Dat['S'],color=cmap(6), edgecolor='none',alpha=alphUn)\n",
    "ax1.fill_between(SS_Dat['Year'],SS_Dat['G']-SS_Dat['S'], SS_Dat['G']+SS_Dat['S'],color=cmap(1), edgecolor='none',alpha=alphUn)\n",
    "ax1.fill_between(LO_Dat['Year'],LO_Dat['G']-LO_Dat['S'], LO_Dat['G']+LO_Dat['S'],color='c', edgecolor='none',alpha=alphUn)\n",
    "ax1.fill_between(CL_Dat['Year'],CL_Dat['G']-CL_Dat['S'], CL_Dat['G']+CL_Dat['S'],color='m', edgecolor='none',alpha=alphUn)\n",
    "ax1.fill_between(HS_Dat['Year'],HS_Dat['G']-HS_Dat['S'], HS_Dat['G']+HS_Dat['S'],color=cmap(7), edgecolor='none',alpha=alphUn)\n",
    "ax1.fill_between(JV_Dat['Year'],JV_Dat['S1'], JV_Dat['S2'],color=cmap(0), edgecolor='none',alpha=alphUn)\n",
    "ax1.fill_between(Ch_Dat['Year'],Ch_Dat['G']-Ch_Dat['S2'], Ch_Dat['G']+Ch_Dat['S1'],color=cmap(2), edgecolor='none',alpha=alphUn)\n",
    "ax1.fill_between(IM_Dat['Year'],IM_Dat['G']-IM_Dat['S'], IM_Dat['G']+IM_Dat['S'],color=cmap(3), edgecolor='none',alpha=alphUn)\n",
    "ax1.fill_between(NZM_Dat['Date'],NZM_Dat['G']-NZM_Dat['S'], NZM_Dat['G']+NZM_Dat['S'],color=cmap(4), edgecolor='none',alpha=0.2)\n",
    "\n",
    "\n",
    "ax1.plot(HS_Dat['Year'],HS_Dat['G'], zorder=1, color=cmap(7), linewidth=LW)\n",
    "ax1.plot(SS_Dat['Year'],SS_Dat['G'], zorder=1, color=cmap(1),linestyle=':',linewidth=LW)\n",
    "ax1.plot(U_Dat['Year'],U_Dat['G'], zorder=1, color=cmap(6),linestyle='--', linewidth=LW)\n",
    "ax1.plot(NZM_Dat['Date'],NZM_Dat['G'], zorder=1, color=cmap(4),linestyle='-.', linewidth=LW)\n",
    "ax1.plot(IM_Dat['Year'],IM_Dat['G'], zorder=1, color=cmap(3),linestyle='--', linewidth=LW)\n",
    "ax1.plot(JV_Dat['Year'],JV_Dat['G'], zorder=1, color=cmap(0), linewidth=LW)\n",
    "ax1.plot(Ch_Dat['Year'],Ch_Dat['G'], zorder=1, color=cmap(2), linewidth=LW,linestyle='--')\n",
    "ax1.plot(WSN_Dat['Year'],WSN_Dat['G'], zorder=1, color='k',linestyle=':', linewidth=LW)\n",
    "\n",
    "ax1.annotate('(a)',xy=(1, 0.99),xycoords='axes fraction',fontsize=60,va='top', ha='right')\n",
    "\n",
    "\n",
    "ax1.set_xlim(left=Y1, right=Y2)\n",
    "ax1.set_ylim(top=19, bottom=0)\n",
    "ax1.xaxis.tick_top()\n",
    "ax1.xaxis.set_label_position('top')\n",
    "ax1.set_ylabel('Group Number')\n",
    "ax1.grid(color=(0.5,0.5,0.5), linestyle='--', linewidth=1, axis='x', which='both')\n",
    "\n",
    "ax2 = fig.add_axes([1.6*ppadh, ppadv+2*ppxy, ppxx, ppxy])\n",
    "\n",
    "#Node Size\n",
    "NZ =50\n",
    "\n",
    "#LineWidth\n",
    "LW = 2\n",
    "\n",
    "nx.draw_networkx_nodes(ObsNtwrk,pos,node_size=15, node_color='k',alpha=0.3, ax = ax2)\n",
    "\n",
    "nx.draw_networkx_nodes(ObsNtwrk.subgraph(Clst[0]),pos,node_size=NZ, node_color='k',alpha=0.2, ax = ax2)\n",
    "nx.draw_networkx_nodes(ObsNtwrk.subgraph(Clst[0]),pos,node_size=NZ,alpha=0.1, node_color='g', ax = ax2)\n",
    "nx.draw_networkx_edges(ObsNtwrk.subgraph(Clst[0]),pos,alpha=0.2, edge_color='g', ax = ax2, width=LW)\n",
    "\n",
    "nx.draw_networkx_nodes(ObsNtwrk.subgraph(Clst[1]),pos,node_size=NZ, node_color='b', ax = ax2)\n",
    "nx.draw_networkx_edges(ObsNtwrk.subgraph(Clst[1]),pos,alpha=0.2, edge_color='b', ax = ax2, width=LW)\n",
    "\n",
    "nx.draw_networkx_nodes(ObsNtwrk.subgraph(Clst[2]),pos,node_size=NZ, node_color='r', ax = ax2)\n",
    "nx.draw_networkx_edges(ObsNtwrk.subgraph(Clst[2]),pos,alpha=0.2, edge_color='r', ax = ax2, width=LW)\n",
    "\n",
    "nx.draw_networkx_nodes(ObsNtwrk.subgraph(Clst[3]),pos,node_size=NZ, node_color='c', ax = ax2)\n",
    "nx.draw_networkx_edges(ObsNtwrk.subgraph(Clst[3]),pos,alpha=0.2, edge_color='c', ax = ax2, width=LW)\n",
    "\n",
    "\n",
    "ax2.set_xlim(left=Y1, right=Y2)\n",
    "ax2.set_yticks([])\n",
    "ax2.set_xticklabels([])\n",
    "ax2.set_ylabel('Networks of\\nOverlaping\\nObservers')\n",
    "ax2.grid(color=(0.5,0.5,0.5), linestyle='--', linewidth=1, axis='x', which='both')\n",
    "\n",
    "ax2.annotate('(b)',xy=(1, 0.99),xycoords='axes fraction',fontsize=60,va='top', ha='right')\n",
    "\n",
    "\n",
    "\n",
    "ax4 = fig.add_axes([1.6*ppadh, ppadv+ppxy, ppxx, ppxy])\n",
    "\n",
    "cmap = plt.get_cmap('Purples')\n",
    "pch = ax4.pcolormesh(YrBl, GrpsBl, np.transpose(GrpsBM), alpha =1, cmap=cmap,edgecolors='None',vmin = -0.5, vmax = np.nanmax(GrpsBM))\n",
    "\n",
    "ax4.set_xlim(left=Y1, right=Y2)\n",
    "ax4.xaxis.tick_top()\n",
    "ax4.set_xticklabels([])\n",
    "ax4.set_ylabel('Current-Series Blended\\nGroup Number')\n",
    "\n",
    "ax4.annotate('(c)',xy=(1, 0.99),xycoords='axes fraction',fontsize=60,va='top', ha='right')\n",
    "\n",
    "\n",
    "\n",
    "ax3 = fig.add_axes([1.6*ppadh, ppadv, ppxx, ppxy])\n",
    "\n",
    "#Area limit for modern data\n",
    "ALim = 10\n",
    "\n",
    "#Marker Size\n",
    "MZ = 10\n",
    "\n",
    "#Font size\n",
    "FZ = 30\n",
    "\n",
    "\n",
    "cmap = plt.cm.get_cmap('Accent')\n",
    "\n",
    "\n",
    "ax3.scatter(BflyHvl['Year'],BflyHvl['Lat'],MZ,alpha=0.5,color=cmap(2))\n",
    "ax3.text( (np.min(BflyHvl['Year'])+np.max(BflyHvl['Year']))/2, 57, 'Hevelius', va='top', ha='center',color=cmap(2), fontsize=FZ)\n",
    "\n",
    "\n",
    "ax3.scatter(BflyMM1['Year'],BflyMM1['Lat'],MZ,alpha=0.5,color=cmap(4))\n",
    "ax3.text( (np.min(BflyMM1['Year'])+np.max(BflyMM1['Year']))/2, -57, 'Malapert, Gassendi\\n& Marcgraf', va='bottom', ha='center',color=cmap(4), fontsize=FZ)\n",
    "\n",
    "\n",
    "ax3.scatter(BflySchn['FRACYEAR'],BflySchn['Lat'],MZ,alpha=0.5,color=cmap(1))\n",
    "ax3.text( (np.min(BflySchn['FRACYEAR'])+np.max(BflySchn['FRACYEAR']))/2+8, 40, 'Scheiner', va='center', ha='center',color=cmap(1), fontsize=FZ)\n",
    "\n",
    "ax3.scatter(BflyGal['FRACYEAR'],BflyGal['Latitude'],MZ,alpha=0.5,color=cmap(7))\n",
    "ax3.text( (np.min(BflyGal['FRACYEAR'])+np.max(BflyGal['FRACYEAR']))/2, 57, 'Galileo', va='top', ha='center',color=cmap(7), fontsize=FZ)\n",
    "\n",
    "ax3.scatter(BflyMM['Year'],BflyMM['Latitude'],MZ,alpha=0.8,color=cmap(6))\n",
    "ax3.text( (np.min(BflyMM['Year'])+np.max(BflyMM['Year']))/2, -57, 'Spoerer and Observers\\nfrom the\\nParis Observatory', va='bottom', ha='center',color=cmap(6), fontsize=FZ)\n",
    "\n",
    "ax3.scatter(BflyStdc['FRACYEAR'],BflyStdc['Lat'],MZ,alpha=0.5,color=cmap(7))\n",
    "ax3.text( (np.min(BflyStdc['FRACYEAR'])+np.max(BflyStdc['FRACYEAR']))/2, 57, 'Staudacher', va='top', ha='center',color=cmap(7), fontsize=FZ)\n",
    "\n",
    "\n",
    "ax3.scatter(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1,BflyMod['CORR_A']>=ALim),'FRACYEAR'],BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1,BflyMod['CORR_A']>=ALim),'LATITUDE'],MZ,alpha=0.5,color=cmap(0))\n",
    "ax3.text( (np.min(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1,BflyMod['CORR_A']>=ALim),'FRACYEAR'])+np.max(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1,BflyMod['CORR_A']>=ALim),'FRACYEAR']))/2, -57, 'Schwabe', va='bottom', ha='center',color=cmap(0), fontsize=FZ)\n",
    "\n",
    "ax3.scatter(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==2,BflyMod['CORR_A']>=ALim),'FRACYEAR'],BflyMod.loc[np.logical_and(BflyMod['SURVEY']==2,BflyMod['CORR_A']>=ALim),'LATITUDE'],MZ,alpha=0.5,color=cmap(4))\n",
    "ax3.text( (np.min(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==2,BflyMod['CORR_A']>=ALim),'FRACYEAR'])+np.max(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==2,BflyMod['CORR_A']>=ALim),'FRACYEAR']))/2, 57, 'Spörer', va='top', ha='center',color=cmap(4), fontsize=FZ)\n",
    "\n",
    "ax3.scatter(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1004,BflyMod['CORR_A']>=ALim),'FRACYEAR'],BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1004,BflyMod['CORR_A']>=ALim),'LATITUDE'],MZ,alpha=0.5,color=cmap(2))\n",
    "ax3.text( (np.min(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1004,BflyMod['CORR_A']>=ALim),'FRACYEAR'])+np.max(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1004,BflyMod['CORR_A']>=ALim),'FRACYEAR']))/2, -57, 'RGO', va='bottom', ha='center',color=cmap(2), fontsize=FZ)\n",
    "\n",
    "ax3.scatter(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1003,BflyMod['CORR_A']>=ALim),'FRACYEAR'],BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1003,BflyMod['CORR_A']>=ALim),'LATITUDE'],MZ,alpha=0.5,color=cmap(5))\n",
    "ax3.text( (np.min(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1003,BflyMod['CORR_A']>=ALim),'FRACYEAR'])+np.max(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1003,BflyMod['CORR_A']>=ALim),'FRACYEAR']))/2-4, 57, 'Debrecen', va='top', ha='center',color=cmap(5), fontsize=FZ)\n",
    "\n",
    "ax3.scatter(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1002,BflyMod['CORR_A']>=ALim),'FRACYEAR'],BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1002,BflyMod['CORR_A']>=ALim),'LATITUDE'],MZ,alpha=0.5,color='k')\n",
    "ax3.text( (np.min(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1002,BflyMod['CORR_A']>=ALim),'FRACYEAR'])+np.max(BflyMod.loc[np.logical_and(BflyMod['SURVEY']==1002,BflyMod['CORR_A']>=ALim),'FRACYEAR']))/2, -57, 'KMAS', va='bottom', ha='center', fontsize=FZ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax3.annotate('(d)',xy=(1, 0.99),xycoords='axes fraction',fontsize=60,va='top', ha='right')\n",
    "\n",
    "ax3.set_xlim(left=Y1, right=Y2)\n",
    "ax3.set_ylim(top=60, bottom=-60)\n",
    "ax3.set_ylabel('Latitude (o)');\n",
    "ax3.grid(color=(0.5,0.5,0.5), linestyle='--', linewidth=1, axis='x', which='both')\n",
    "\n",
    "\n",
    "fig.savefig('F1_Solar_Cycle_Obs_Data.png',dpi=figDPI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Matrix containing all observations in a window of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maunder Minimum\n",
    "\n",
    "Y1 = 1634  # First year of the window of interest\n",
    "Y2 = 1727  # Second year of the window of interest\n",
    "\n",
    "# Find all observers with at least one day in the window of interest\n",
    "UnqObs = GN_Dat.loc[np.logical_and(np.logical_and(GN_Dat.YEAR>=Y1, GN_Dat.YEAR<=Y2),GN_Dat.STATION>0), 'STATION'].values\n",
    "UnqObs = np.unique(UnqObs)\n",
    "\n",
    "# Finding the ordinal days that contain these observers completely\n",
    "OD1 = 9999999\n",
    "OD2 = 0\n",
    "for Obs in UnqObs:\n",
    "    OD1 = np.min(np.append(GN_Dat.loc[GN_Dat.STATION == Obs, 'ORDINAL'].values, OD1))\n",
    "    OD2 = np.max(np.append(GN_Dat.loc[GN_Dat.STATION == Obs, 'ORDINAL'].values, OD2))\n",
    "    \n",
    "# Create Matrix for observations\n",
    "ObsMat = np.zeros([OD2-OD1+1, UnqObs.shape[0]])*np.nan\n",
    "\n",
    "# Fill it with observations\n",
    "for i in range(0,UnqObs.shape[0]):\n",
    "    ObsData = GN_Dat.loc[GN_Dat.STATION == UnqObs[i], ['ORDINAL','GROUPS']].values\n",
    "    ObsMat[ObsData[:,0].astype(int)-OD1,i] = ObsData[:,1]\n",
    "    \n",
    "    \n",
    "## Modern period\n",
    "\n",
    "Y12 = 1865  # First year of the window of interest\n",
    "Y22 = Y12+Y2-Y1  # Second year of the window of interest\n",
    "\n",
    "# Find all observers with at least one day in the window of interest\n",
    "UnqObs2 = GN_Dat.loc[np.logical_and(np.logical_and(GN_Dat.YEAR>=Y12, GN_Dat.YEAR<=Y22),GN_Dat.STATION>0), 'STATION'].values\n",
    "UnqObs2 = np.unique(UnqObs2)\n",
    "\n",
    "# Finding the ordinal days that contain these observers completely\n",
    "OD12 = 9999999\n",
    "OD22 = 0\n",
    "for Obs in UnqObs2:\n",
    "    OD12 = np.min(np.append(GN_Dat.loc[GN_Dat.STATION == Obs, 'ORDINAL'].values, OD12))\n",
    "    OD22 = np.max(np.append(GN_Dat.loc[GN_Dat.STATION == Obs, 'ORDINAL'].values, OD22))\n",
    "    \n",
    "# Create Matrix for observations\n",
    "ObsMat2 = np.zeros([OD22-OD12+1, UnqObs2.shape[0]])*np.nan\n",
    "\n",
    "# Fill it with observations\n",
    "for i in range(0,UnqObs2.shape[0]):\n",
    "    ObsData2 = GN_Dat.loc[GN_Dat.STATION == UnqObs2[i], ['ORDINAL','GROUPS']].values\n",
    "    ObsMat2[ObsData2[:,0].astype(int)-OD12,i] = ObsData2[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate activity mask based on observation matrix and Yearly Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maunder Minimum\n",
    "\n",
    "# Add observations of all observers to identify active days\n",
    "ADmsk = (np.nansum(ObsMat,axis=1)>0).astype(float) + 1\n",
    "\n",
    "# Calculate the number of NaNs each day to identify missing days\n",
    "MDmsk = np.sum(np.isnan(ObsMat),axis=1)==UnqObs.shape[0]\n",
    "\n",
    "# Fill days with missing observations with NaN\n",
    "ADmsk[MDmsk] = ADmsk[MDmsk]-1\n",
    "\n",
    "# Creating fractional year variable\n",
    "\n",
    "# Ordinal to fractional\n",
    "def ordinal2fracy(Ord):\n",
    "    YEAR = datetime.date.fromordinal(Ord).year     \n",
    "    return YEAR + ( Ord - datetime.date(YEAR,1,1).toordinal() )/(datetime.date(YEAR+1,1,1).toordinal() - datetime.date(YEAR,1,1).toordinal() )\n",
    "\n",
    "vordinal2fracy=np.vectorize(ordinal2fracy)\n",
    "fracyr = vordinal2fracy(np.arange(OD1,OD2+1))\n",
    "\n",
    "\n",
    "\n",
    "## Modern Period\n",
    "\n",
    "# Add observations of all observers to identify active days\n",
    "ADmsk2 = (np.nansum(ObsMat2,axis=1)>0).astype(float) + 1\n",
    "\n",
    "# Calculate the number of NaNs each day to identify missing days\n",
    "MDmsk2 = np.sum(np.isnan(ObsMat2),axis=1)==UnqObs2.shape[0]\n",
    "\n",
    "# Fill days with missing observations with NaN\n",
    "ADmsk2[MDmsk2] = ADmsk2[MDmsk2]-1\n",
    "\n",
    "# Creating fractional year variable\n",
    "fracyr2 = vordinal2fracy(np.arange(OD12,OD22+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unify Butterfly diagram observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area limit for modern data\n",
    "ALim = 10\n",
    "\n",
    "# Modern data\n",
    "BflyAll = BflyMod.copy()\n",
    "BflyAll.loc[BflyAll['CORR_A']<ALim,'LATITUDE'] = np.NaN\n",
    "BflyAll = BflyAll.loc[np.isfinite(BflyAll['CORR_A']),('FRACYEAR','ORDINAL','LATITUDE')].values\n",
    "\n",
    "# Hevelius\n",
    "tmp = BflyHvl.loc[:,('Year','Lat')]\n",
    "tmp['ORDINAL'] = tmp.apply(lambda x: np.round(datetime.date(np.floor(x['Year']).astype(int),1,1).toordinal()\n",
    "                           + (x['Year']-np.floor(x['Year']))*(datetime.date(np.floor(x['Year']).astype(int)+1,1,1).toordinal()\n",
    "                                                             -datetime.date(np.floor(x['Year']).astype(int),1,1).toordinal())).astype(int)\n",
    "                                  ,axis=1)\n",
    "tmp = tmp.loc[:,('Year','ORDINAL','Lat')].values\n",
    "BflyAll = np.append(BflyAll,tmp,axis=0)\n",
    "\n",
    "# Galileo\n",
    "tmp = BflyGal.loc[:,('FRACYEAR','ORDINAL','Latitude')].values\n",
    "BflyAll = np.append(BflyAll,tmp,axis=0)\n",
    "\n",
    "# Scheiner\n",
    "tmp = BflySchn.loc[:,('FRACYEAR','ORDINAL','Lat')].values\n",
    "BflyAll = np.append(BflyAll,tmp,axis=0)\n",
    "\n",
    "# Harriot\n",
    "tmp = BflyHrt.loc[:,('FRACYEAR','ORDINAL','Lat')].values\n",
    "BflyAll = np.append(BflyAll,tmp,axis=0)\n",
    "\n",
    "# Staudacher\n",
    "tmp = BflyStdc.loc[:,('FRACYEAR','ORDINAL','Lat')].values\n",
    "BflyAll = np.append(BflyAll,tmp,axis=0)\n",
    "\n",
    "# Sporer and Ribes\n",
    "tmp = BflyMM.loc[:,('Year','Latitude')]\n",
    "tmp['ORDINAL'] = tmp.apply(lambda x: np.round(datetime.date(np.floor(x['Year']).astype(int),1,1).toordinal()\n",
    "                           + (x['Year']-np.floor(x['Year']))*(datetime.date(np.floor(x['Year']).astype(int)+1,1,1).toordinal()\n",
    "                                                             -datetime.date(np.floor(x['Year']).astype(int),1,1).toordinal())).astype(int)\n",
    "                                  ,axis=1)\n",
    "tmp = tmp.loc[:,('Year','ORDINAL','Latitude')].values\n",
    "BflyAll = np.append(BflyAll,tmp,axis=0)\n",
    "\n",
    "# Malapert, and so on\n",
    "tmp = BflyMM1.loc[:,('Year','Lat')]\n",
    "\n",
    "\n",
    "tmp['ORDINAL'] = tmp.apply(lambda x: np.round(datetime.date(np.floor(x['Year']).astype(int),1,1).toordinal()\n",
    "                           + (x['Year']-np.floor(x['Year']))*(datetime.date(np.floor(x['Year']).astype(int)+1,1,1).toordinal()\n",
    "                                                             -datetime.date(np.floor(x['Year']).astype(int),1,1).toordinal())).astype(int)\n",
    "                                  ,axis=1)\n",
    "tmp = tmp.loc[:,('Year','ORDINAL','Lat')].values\n",
    "BflyAll = np.append(BflyAll,tmp,axis=0)\n",
    "\n",
    "# Create pandas dataframe\n",
    "BflyAll = pd.DataFrame(BflyAll, columns=['FRACYEAR','ORDINAL','LATITUDE'])\n",
    "BflyAll['ORDINAL'] = BflyAll['ORDINAL'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Open a file and use dump()\n",
    "with open(\"BflyAll.pkl\", \"wb\") as file:\n",
    "\n",
    "    # A new file will be created\n",
    "    pickle.dump(BflyAll, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating coverage of sunspot group observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define years to calculate butterfly diagram coverage\n",
    "Y1 = 1600\n",
    "Y2 = 1900\n",
    "\n",
    "# Number of years in coverage window\n",
    "YrCum = 2\n",
    "\n",
    "# Removing days wihout observers\n",
    "CvrObs = GN_Dat['ORDINAL'].copy()\n",
    "CrvObs = CvrObs[GN_Dat['STATION']>0]\n",
    "\n",
    "# Identifying each day with an observation\n",
    "CrvObs = np.unique(CrvObs)\n",
    "\n",
    "# Calculate the corresponding fractional year\n",
    "CrvObsFy = vordinal2fracy(CrvObs)\n",
    "\n",
    "# Set up repository variables\n",
    "CrvObsYr = np.arange(Y1,Y2+1,YrCum)\n",
    "CrvObsCv = CrvObsYr.copy().astype(float)*0\n",
    "\n",
    "for i in np.arange(0,CrvObsYr.shape[0]):\n",
    "    # Calculate number of days in year\n",
    "    NdaysYr = datetime.date(CrvObsYr[i]+YrCum,1,1).toordinal()-datetime.date(CrvObsYr[i],1,1).toordinal()\n",
    "    \n",
    "    # Find number of observations in first year of the bin\n",
    "    NdaysOb = np.sum(np.floor(CrvObsFy)==CrvObsYr[i])\n",
    "    \n",
    "    # Add other years to the bin\n",
    "    for j in np.arange(1,YrCum):\n",
    "        NdaysOb = NdaysOb + np.sum(np.floor(CrvObsFy)==CrvObsYr[i]+j)\n",
    "    \n",
    "    # Calculate coverage\n",
    "    CrvObsCv[i] = NdaysOb/NdaysYr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate observational coverage based on butterfly diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = 1600\n",
    "Y2 = 1980\n",
    "\n",
    "# Calculate a unique list of days represented in the butterfly diagram\n",
    "ObsDsBfly = np.unique(BflyAll['ORDINAL'])\n",
    "    \n",
    "# Calculate the corresponding fractional year\n",
    "BfObsFy = vordinal2fracy(ObsDsBfly)\n",
    "\n",
    "# Set up repository variables\n",
    "BfObsYr = np.arange(Y1,Y2+1,YrCum)\n",
    "BfObsCv = BfObsYr.copy().astype(float)*0\n",
    "\n",
    "for i in np.arange(0,BfObsYr.shape[0]):\n",
    "    # Calculate number of days in year\n",
    "    NdaysYr = datetime.date(BfObsYr[i]+YrCum,1,1).toordinal()-datetime.date(BfObsYr[i],1,1).toordinal()\n",
    "    \n",
    "    # Find number of observations in first year of the bin\n",
    "    NdaysOb = np.sum(np.floor(BfObsFy)==BfObsYr[i])\n",
    "    \n",
    "    # Add other years to the bin\n",
    "    for j in np.arange(1,YrCum):\n",
    "        NdaysOb = NdaysOb + np.sum(np.floor(BfObsFy)==BfObsYr[i]+j)\n",
    "    \n",
    "    # Calculate coverage\n",
    "    BfObsCv[i] = NdaysOb/NdaysYr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"BfObsYr.pkl\", \"wb\") as file:\n",
    "    pickle.dump(BfObsYr, file)\n",
    "\n",
    "with open(\"BfObsCv.pkl\", \"wb\") as file:\n",
    "    pickle.dump(BfObsCv, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-term historic data plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning a 'Source' to each DataFrame\n",
    "SS_Dat[\"Source\"] = \"Svalgaard & Schatten (2015)\"\n",
    "U_Dat[\"Source\"] = \"Willamo et al. (2017)\"\n",
    "IM_Dat[\"Source\"] = \"Ivanov & Miletsky (2017)\"\n",
    "Ch_Dat[\"Source\"] = \"Chatzistergos et al. (2017)\"\n",
    "JV_Dat[\"Source\"] = \"Vaquero et al. (2015)\"\n",
    "\n",
    "# Concatenating the DataFrames\n",
    "data = pd.concat([SS_Dat, U_Dat, IM_Dat, Ch_Dat, JV_Dat], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Open a file and use dump()\n",
    "with open(\"data_a.pkl\", \"wb\") as file:\n",
    "\n",
    "    # A new file will be created\n",
    "    pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_df_b = pd.DataFrame(\n",
    "    {\n",
    "        \"YearStart\": CrvObsYr,\n",
    "        \"YearEnd\": CrvObsYr\n",
    "        + YrCum,  \n",
    "        \"Coverage\": CrvObsCv,\n",
    "  \n",
    "        \"Color\": [clr_based_on_BfObsCv(i) for i in CrvObsCv], \n",
    "        \"Alpha\": [alpha_based_on_BfObsCv(i) for i in CrvObsCv], \n",
    "        \"Ymin\": -20,\n",
    "        \"Ymax\": 20,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file and use dump()\n",
    "with open(\"data_b.pkl\", \"wb\") as file:\n",
    "\n",
    "    # A new file will be created\n",
    "    pickle.dump(mask_df_b, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_df = pd.DataFrame(\n",
    "    {\n",
    "        \"YearStart\": BfObsYr,\n",
    "        \"YearEnd\": BfObsYr + YrCum,  \n",
    "        \"Coverage\": BfObsCv,\n",
    "        \"Color\": [clr_based_on_BfObsCv(i) for i in BfObsCv],  \n",
    "        \"Alpha\": [alpha_based_on_BfObsCv(i) for i in BfObsCv], \n",
    "        \"Ymin\":-80,\n",
    "        \"Ymax\":80\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file and use dump()\n",
    "with open(\"data_c.pkl\", \"wb\") as file:\n",
    "\n",
    "    # A new file will be created\n",
    "    pickle.dump(mask_df, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "511px",
    "left": "1546px",
    "right": "20px",
    "top": "237px",
    "width": "368px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
